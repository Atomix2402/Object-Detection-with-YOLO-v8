{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6927,"databundleVersionId":45059,"sourceType":"competition"}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Video Link: https://www.youtube.com/watch?v=IHq1t7NxS8k&t=1450s","metadata":{}},{"cell_type":"markdown","source":"## **Unzipping data**","metadata":{}},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms.functional as TF\nfrom torch.utils.data import Dataset\nimport numpy as np\nfrom torch.utils.data import DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom tqdm import tqdm # progress bar\nimport torch.optim as optim","metadata":{"execution":{"iopub.status.busy":"2024-02-26T10:07:29.935213Z","iopub.execute_input":"2024-02-26T10:07:29.935505Z","iopub.status.idle":"2024-02-26T10:07:38.835119Z","shell.execute_reply.started":"2024-02-26T10:07:29.935478Z","shell.execute_reply":"2024-02-26T10:07:38.834320Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"os.makedirs('/kaggle/working/TrainPhotos')\nos.makedirs('/kaggle/working/ValidPhotos')","metadata":{"execution":{"iopub.status.busy":"2024-02-26T10:07:46.268381Z","iopub.execute_input":"2024-02-26T10:07:46.269554Z","iopub.status.idle":"2024-02-26T10:07:46.279657Z","shell.execute_reply.started":"2024-02-26T10:07:46.269514Z","shell.execute_reply":"2024-02-26T10:07:46.273452Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Training dataset","metadata":{}},{"cell_type":"code","source":"#unzipping photos\npath_to_zip_file = \"/kaggle/input/carvana-image-masking-challenge/train.zip\"\ndirectory_to_extract_to = \"/kaggle/working/TrainPhotos\"\n\nimport zipfile\nwith zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n  zip_ref.extractall(directory_to_extract_to)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T10:07:47.525028Z","iopub.execute_input":"2024-02-26T10:07:47.525722Z","iopub.status.idle":"2024-02-26T10:07:54.529805Z","shell.execute_reply.started":"2024-02-26T10:07:47.525689Z","shell.execute_reply":"2024-02-26T10:07:54.528872Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# unzipping masks\npath_to_zip_file = \"/kaggle/input/carvana-image-masking-challenge/train_masks.zip\"\ndirectory_to_extract_to = \"/kaggle/working/TrainPhotos\"\n\nimport zipfile\nwith zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n  zip_ref.extractall(directory_to_extract_to)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T10:07:54.531776Z","iopub.execute_input":"2024-02-26T10:07:54.532625Z","iopub.status.idle":"2024-02-26T10:07:55.373169Z","shell.execute_reply.started":"2024-02-26T10:07:54.532586Z","shell.execute_reply":"2024-02-26T10:07:55.372422Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"For validation dataset","metadata":{}},{"cell_type":"code","source":"os.makedirs('/kaggle/working/ValidPhotos/valid')\nos.makedirs('/kaggle/working/ValidPhotos/valid_masks')","metadata":{"execution":{"iopub.status.busy":"2024-02-26T10:07:55.374172Z","iopub.execute_input":"2024-02-26T10:07:55.374449Z","iopub.status.idle":"2024-02-26T10:07:55.379222Z","shell.execute_reply.started":"2024-02-26T10:07:55.374419Z","shell.execute_reply":"2024-02-26T10:07:55.378357Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_photos_path = \"/kaggle/working/TrainPhotos/train\"\ntrain_masks_path = \"/kaggle/working/TrainPhotos/train_masks\"\n\n\nvalid_photos_path = \"/kaggle/working/ValidPhotos/valid\"\nvalid_masks_path = \"/kaggle/working/ValidPhotos/valid_masks\"\n\n\n\nphotos = os.listdir(train_photos_path)\n\n\nvalid_count = int(len(photos) * 0.2)\nvalid_photos = photos[:valid_count]\n\n\nfor photo in valid_photos:\n    \n    im = Image.open(os.path.join(train_photos_path, photo))\n    im = im.convert(\"RGB\")\n    jpg_path = os.path.join(valid_photos_path, os.path.splitext(photo)[0] + \".jpg\")\n    im.save(jpg_path)\n    \n    \n    mask_name = os.path.splitext(photo)[0] + '_mask.gif'\n    mask_path = os.path.join(train_masks_path, mask_name)\n    \n    \n    mask_im = Image.open(mask_path)\n    mask_im = mask_im.convert(\"RGB\")\n    jpg_mask_path = os.path.join(valid_masks_path, os.path.splitext(mask_name)[0] + \".gif\")\n    mask_im.save(jpg_mask_path)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T10:07:55.381816Z","iopub.execute_input":"2024-02-26T10:07:55.382222Z","iopub.status.idle":"2024-02-26T10:11:17.667207Z","shell.execute_reply.started":"2024-02-26T10:07:55.382192Z","shell.execute_reply":"2024-02-26T10:11:17.666252Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"# this class acts as out dataset creator in pytorch, \n# we basically have to do it in pytorch its like mandatory thing.\n# To create our dataset class and to get this in our dataloader\n\nclass CarvanaDataset(Dataset):\n    def __init__(self,image_dir,mask_dir,transform = None):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.transform = transform\n        self.images = os.listdir(image_dir)\n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self,index):\n        img_path = os.path.join(self.image_dir,self.images[index])\n        mask_path = os.path.join(self.mask_dir,self.images[index].replace(\".jpg\",\"_mask.gif\"))\n        \n        image = np.array(Image.open(img_path).convert(\"RGB\")) \n        # we are using np array because we are going to use albumentations library for\n        # data augmentation. so PIL has to be converted to np array\n        \n        mask = np.array(Image.open(mask_path).convert(\"L\"),dtype = np.float32)\n        # mask is grayscale so we are converting to L\n        # 0.0 or 255 black and white respectively\n        \n        mask[mask == 255.0] = 1.0\n        # we are using sigmoid as our last activation to label it nicely we are \n        # normalizing it to 1 and 0\n        \n        if self.transform is not None:\n            augmentations = self.transform(image=image, mask=mask)\n            image = augmentations[\"image\"]\n            mask = augmentations[\"mask\"]\n        return image,mask\n            \n        \n        \n        \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2024-02-26T10:11:17.668406Z","iopub.execute_input":"2024-02-26T10:11:17.668715Z","iopub.status.idle":"2024-02-26T10:11:17.677830Z","shell.execute_reply.started":"2024-02-26T10:11:17.668690Z","shell.execute_reply":"2024-02-26T10:11:17.676744Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Creating Model","metadata":{}},{"cell_type":"code","source":"# Image segmentation tutorial with U network from scratch !!!\n# creating U net based on the 2015 paper with slight variations i have noted them in the code\nclass DoubleConv(nn.Module):\n    def __init__(self,in_channels,out_channels):\n        super(DoubleConv,self).__init__()\n        self.conv = nn.Sequential(\n            # Same convolution means when you pad, the output size is the same as the input size.\n            # Basically you pad,let's say a 6 by 6 image in such a way\n            # that the output should also be a 6 by 6 image.\n            # bias is false cuz we are using BatchNormalization based on the 2016 paper\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n    def forward(self,x):\n        return self.conv(x)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T10:11:17.678948Z","iopub.execute_input":"2024-02-26T10:11:17.679251Z","iopub.status.idle":"2024-02-26T10:11:17.692152Z","shell.execute_reply.started":"2024-02-26T10:11:17.679227Z","shell.execute_reply":"2024-02-26T10:11:17.691308Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class UNET(nn.Module):\n    def __init__(self,in_channels = 3,out_channels = 1, features = [64,128,256,512] ):\n        super(UNET,self).__init__()\n        self.downs = nn.ModuleList()\n        self.ups = nn.ModuleList()\n        self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\n        # the pooling layer will floor the size of height and width when divided like\n        # 161x161 when going down -> 80x80 -> becomes 160x160,\n        # so yes we can't concatenate this way that's why we must add padding\n        # i'll add a notes file for this one point [1]\n\n        # Downward Part of UNET\n        for feature in features:\n            self.downs.append(DoubleConv(in_channels,feature))\n            # we defined doubleconv layer  above as 2 conv layers\n            # together in each step of u-net.\n            in_channels = feature\n\n        # Bottle Neck layer in UNET in = 512 and out = 1024\n        self.bottleneck = DoubleConv(features[-1],features[-1]*2) # it is saved as a method\n        self.final_conv = nn.Conv2d(features[0],out_channels,kernel_size=1)\n\n        # Upward Part of UNET\n        for feature in reversed(features):\n            self.ups.append(\n                nn.ConvTranspose2d(\n                    feature*2,feature,kernel_size=2,stride=2,\n                )\n            )\n            self.ups.append(DoubleConv(feature*2,feature))\n    def forward(self,x):\n        skip_connections = [] # we will store all the skipped connections\n        # first we are going down.\n        # go down and then collect skipped connections and then pool\n        for down in self.downs:\n            x = down(x)\n            skip_connections.append(x)\n            x = self.pool(x)\n        # Then apply the bottleneck layer\n        x = self.bottleneck(x)\n        # reverse the skipped connections collected when going down\n        skip_connections = skip_connections[::-1]\n        # Then from the bottleneck,\n        # we go upwards collect skipped connections and concatenate\n        # then go up again.\n        for idx in range(0,len(self.ups),2):\n            x = self.ups[idx](x)\n            skip_connection = skip_connections[idx//2] # // refers to integer division\n\n            if x.shape != skip_connection.shape:\n                x = TF.resize(x, size=skip_connection.shape[2:])\n                # we are only taking height and width and skipping batch_size and num of channels\n\n            concat_skip = torch.cat((skip_connection,x),dim = 1)\n            x = self.ups[idx+1](concat_skip)\n        # then we deliver the resulted features from the conv to the final convolution layer\n        # and return it.\n        return self.final_conv(x)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-26T10:11:17.693303Z","iopub.execute_input":"2024-02-26T10:11:17.693671Z","iopub.status.idle":"2024-02-26T10:11:17.707064Z","shell.execute_reply.started":"2024-02-26T10:11:17.693648Z","shell.execute_reply":"2024-02-26T10:11:17.706240Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Training part","metadata":{}},{"cell_type":"code","source":"# Hyperparameters\nLEARNING_RATE = 1e-4\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 32\nNUM_EPOCHS = 3\nNUM_WORKERS = 2\nIMAGE_HEIGHT = 160 # 1280 originally\nIMAGE_WIDTH = 240  # 1918 originally\nPIN_MEMORY = True\nLOAD_MODEL = True\nTRAIN_IMG_DIR = \"/kaggle/working/TrainPhotos/train\"\nTRAIN_MASK_DIR = \"/kaggle/working/TrainPhotos/train_masks\"\nVAL_IMG_DIR = \"/kaggle/working/ValidPhotos/valid\"\nVAL_MASK_DIR = \"/kaggle/working/ValidPhotos/valid_masks\"","metadata":{"execution":{"iopub.status.busy":"2024-02-26T10:40:48.777489Z","iopub.execute_input":"2024-02-26T10:40:48.777815Z","iopub.status.idle":"2024-02-26T10:40:48.783637Z","shell.execute_reply.started":"2024-02-26T10:40:48.777791Z","shell.execute_reply":"2024-02-26T10:40:48.782761Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# train function will do 1 epoch of training\n# we will use tqdm here for the progress bar\ndef train_fn(loader,model,optimizer,loss_fn,scaler):\n    loop = tqdm(loader)\n    \n    for batch_idx,(data,targets) in enumerate(loop):\n        data = data.to(device = DEVICE)\n        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n        \n        # forward pass\n        #we will use float 16 training check the video for that\n        with torch.cuda.amp.autocast():\n            predictions = model(data)\n            loss = loss_fn(predictions,targets)\n        \n        # backward pass\n        optimizer.zero_grad() # zero all the gradients from previous\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        # uodate tqdm loop\n        loop.set_postfix(loss = loss.item())\n        ","metadata":{"execution":{"iopub.status.busy":"2024-02-26T10:11:17.774740Z","iopub.execute_input":"2024-02-26T10:11:17.775008Z","iopub.status.idle":"2024-02-26T10:11:17.786661Z","shell.execute_reply.started":"2024-02-26T10:11:17.774982Z","shell.execute_reply":"2024-02-26T10:11:17.785842Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"os.mkdir(\"resultPhotos\")","metadata":{"execution":{"iopub.status.busy":"2024-02-26T10:11:17.789176Z","iopub.execute_input":"2024-02-26T10:11:17.789465Z","iopub.status.idle":"2024-02-26T10:11:17.795737Z","shell.execute_reply.started":"2024-02-26T10:11:17.789434Z","shell.execute_reply":"2024-02-26T10:11:17.795091Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n    \ndef load_checkpoint(chekcpoint, model):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n\ndef get_loaders(train_dir,train_maskdir,val_dir,val_maskdir,batch_size,\n                train_transform,val_transform,num_workers=4,pin_memory=True,):\n    train_ds = CarvanaDataset(\n        image_dir = train_dir,\n        mask_dir = train_maskdir,\n        transform = train_transform,\n    )\n    \n    train_loader = DataLoader(\n        train_ds,\n        batch_size = batch_size,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        shuffle=True\n    )\n    \n    \n    val_ds = CarvanaDataset(\n        image_dir = val_dir,\n        mask_dir = val_maskdir,\n        transform = val_transform\n    )\n    \n    val_loader = DataLoader(\n        val_ds,\n        batch_size = batch_size,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        shuffle=False,\n    )\n    \n    return train_loader, val_loader\n\ndef check_accuracy(loader,model,device=\"cuda\"):\n    num_correct = 0\n    num_pixels = 0 \n    dice_score = 0\n    # for segmentation we are outputing a prediction \n    # for each individual pixel for each class\n    model.eval()\n    \n    with torch.no_grad():\n        for x,y in loader:\n            x = x.to(device)\n            y = y.to(device).unsqueeze(1)\n            preds = torch.sigmoid(model(x))\n            preds = (preds>0.5).float() # since its a binary segmentation\n            # for more classes we need to adapt the check accuracy method accordingly\n            num_correct += (preds == y).sum()\n            num_pixels += torch.numel(preds) # number of elements in preds\n            dice_score +=(2*(preds*y).sum())/((preds+y).sum() + 1e-8)\n    print(\n        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\"\n    )\n    print(f\"Dice score: {dice_score/len(loader)}\")\n    # there are better metrics compared to accuracy\n    # acc is flawed if we just output black pixels it will have output of greater than 80%\n    \n    \ndef save_predictions_as_imgs(loader, model, folder=\"resultPhotos/\", device=\"cuda\"):\n    model.eval()\n    for idx, (x, y) in enumerate(loader):\n        x = x.to(device=device)\n        with torch.no_grad():\n            preds = torch.sigmoid(model(x))\n            preds = (preds > 0.5).float()\n        torchvision.utils.save_image(\n            preds, f\"{folder}/pred_{idx}.png\"\n        )\n        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}\")    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-02-26T10:39:40.422849Z","iopub.execute_input":"2024-02-26T10:39:40.423232Z","iopub.status.idle":"2024-02-26T10:39:40.438455Z","shell.execute_reply.started":"2024-02-26T10:39:40.423195Z","shell.execute_reply":"2024-02-26T10:39:40.437463Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Accuracy vs. Dice Score in Binary Segmentation:\n\nAccuracy:\n\n**What it measures:** Accuracy is a metric that calculates the overall correctness of predictions, considering both true positives and true negatives.\n\n**Why it may not be ideal:** In binary segmentation tasks, where you're separating an image into two parts (e.g., object vs. background), accuracy can be misleading if one class is much smaller than the other. If you have a lot of background pixels and only a few object pixels, a model predicting all pixels as background can still have a high accuracy.\n\n**Dice Score:**\n\n**What it measures:** The Dice coefficient specifically looks at the overlap between what your model predicts as the object and what is actually the object in the real data.\n\n**Why it's useful:** It's better suited for imbalanced situations. If your model correctly identifies the small object region, even if there are many more background pixels, the Dice score will still reflect the good performance.\n\n**Example:**\nImagine you're looking at medical images to find tumors. If a tumor is tiny compared to the healthy tissue, accuracy might be high even if your model is missing the tumors. Dice score, on the other hand, cares about how well your model finds the actual tumor region, making it more sensitive to the performance on the small but crucial areas.\n\nIn summary, in binary segmentation tasks, where one class is much smaller than the other, the Dice score gives you a better picture of how well your model is doing on the important regions you care about, especially when accuracy might be misleading due to class imbalance.","metadata":{}},{"cell_type":"markdown","source":"## Main class (need to be in last)","metadata":{}},{"cell_type":"code","source":"def main():\n    train_transform = A.Compose([\n        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n        A.Rotate(limit=35,p=1.0),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.1),\n        A.Normalize(\n        mean=[0.0,0.0,0.0],\n        std=[1.0,1.0,1.0],\n        max_pixel_value=255.0,\n        ),\n        ToTensorV2(),\n    ],)\n    val_transforms = A.Compose([\n        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n        A.Normalize(\n        mean=[0.0,0.0,0.0],\n        std=[1.0,1.0,1.0],\n        max_pixel_value=255.0,\n        ),\n        ToTensorV2(),\n    ],)\n    model = UNET(in_channels = 3,out_channels=1).to(DEVICE)\n    loss_fn = nn.BCEWithLogitsLoss() \n    \"\"\"\n    \n    * LogitsLoss is used since we did not keep sigmoid function in the output of model\n    * If you have multi class segmentation we change out_channels to different number\n    loss function to cross entropy loss\n    * Ours is binary segmentation black or white mask rn\n    \n    \"\"\"\n    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n    train_loader,val_loader = get_loaders(\n        TRAIN_IMG_DIR,\n        TRAIN_MASK_DIR,\n        VAL_IMG_DIR,\n        VAL_MASK_DIR,\n        BATCH_SIZE,\n        train_transform,\n        val_transforms,\n        NUM_WORKERS,\n        PIN_MEMORY,\n    )\n    scaler = torch.cuda.amp.GradScaler()\n    for epoch in range(NUM_EPOCHS):\n        train_fn(train_loader,model,optimizer,loss_fn,scaler)\n        \n        #save model\n        checkpoint = {\n            \"state_dict\": model.state_dict(),\n            \"optimizer\": optimizer.state_dict()\n        }\n        save_checkpoint(checkpoint)\n        \n        # check accuracy\n        check_accuracy(val_loader, model, device=DEVICE)\n        \n        # print some examples\n        \"\"\"save_predictions_as_imgs(\n            val_loader, model, folder=\"resultPhotos/\", device=DEVICE\n        )\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-02-26T10:40:53.225577Z","iopub.execute_input":"2024-02-26T10:40:53.226283Z","iopub.status.idle":"2024-02-26T10:40:53.236502Z","shell.execute_reply.started":"2024-02-26T10:40:53.226253Z","shell.execute_reply":"2024-02-26T10:40:53.235566Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"main()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T10:40:53.394113Z","iopub.execute_input":"2024-02-26T10:40:53.394361Z","iopub.status.idle":"2024-02-26T10:46:20.372496Z","shell.execute_reply.started":"2024-02-26T10:40:53.394340Z","shell.execute_reply":"2024-02-26T10:46:20.371248Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"100%|██████████| 159/159 [01:30<00:00,  1.75it/s, loss=0.222]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\nGot 38628870/39052800 with acc 98.91\nDice score: 0.9746503233909607\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 159/159 [01:30<00:00,  1.76it/s, loss=0.0876]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\nGot 37789080/39052800 with acc 96.76\nDice score: 0.9279659986495972\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 159/159 [01:31<00:00,  1.74it/s, loss=0.046] \n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\nGot 38512778/39052800 with acc 98.62\nDice score: 0.9678897261619568\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}